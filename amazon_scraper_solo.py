# -*- coding: utf-8 -*-
"""Amazon Scraper Solo

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14JnNa0XmIWKI457bhdMxFFr0GQNoktQy
"""

#!pip install requests
#!pip install asyncio
#!pip install nest_asyncio
#!pip install aiohttp

from bs4 import BeautifulSoup as soup
import threading, requests, random, re, json
from os import path

class file_handle():
    def write_in(self, file_name : str,data, seperator=None):
        if seperator ==None:
                seperator = ','

        file = open(file_name, 'a', encoding="utf-8", errors='ignore')
        ttype = type(data)
        if ttype ==  str:
            file.write(data)
            file.write(seperator)

        elif ttype == dict:
            d = json.dumps(data)
            file.write(d)
            file.write(seperator)

        elif ttype == list or ttype == tuple or ttype == set:
            for d in data:
                file.write(d)
                file.write(seperator)
        file.close()

    def write_file(self, file_name : str,data, seperator=None):
        file = open(file_name,'w', encoding="utf-8",errors='ignore')
        ttype = type(data)
        if ttype ==  str:
            file.write(data)
            if seperator ==None:
                file.write(',')
            else:
                file.write(seperator)
        elif ttype == dict:
            json.dump(data,file)
            if seperator ==None:
                file.write(';')
            else:
                file.write(seperator)
        elif ttype == list or ttype == tuple or ttype == set:
            for d in data:
                file.write(d)
                if seperator ==None:
                    file.write(',')
                else:
                    file.write(seperator)
        file.close()

    def read_file(self, file_name : str, spliter = None):
        file = open(file_name,'r',encoding='utf-8',errors='ignore')
        data = file.read()
        if spliter != None:
            data = data.split(spliter)
        file.close()
        return data

file_handle = file_handle()

file_loca = '/content/drive/MyDrive/Colab Notebooks/storage'     #location where all the files are stored
file_asinList = file_loca + '/asinList.txt'
file_product_url = file_loca + '/productUrls.txt'
file_product_Data = file_loca + '/product_data.json'
file_pagination_Data = file_loca + '/pagination_list.json'
file_searched_products = file_loca + '/search_data.json'
file_searchPage_List = file_loca + '/searchKeywords.txt'
file_searchedList = file_loca + '/searchedList.txt'
file_search_page = file_loca + '/searchedPg.json'

asins_list=[]
# gets all user agents and coverts the list into set for efficiency
user_agents = list(file_handle.read_file(file_loca+'/user-agent.txt',"\n"))
print('user_agents total:' + str( len(user_agents) ))


# list of proxie
proxy = list(file_handle.read_file(file_loca+'/proxies.txt',"\n"))
print('Proxies total:' + str( len(proxy) ))

# gets all the queries for searching
search_keys = list(file_handle.read_file(file_searchPage_List,','))
print('search_keys total:' + str( len(search_keys) ))

# gets all the urls the have been scraped

searchedUrls = list(file_handle.read_file(file_searchedList,','))
print('searchedUrls total:' + str( len(searchedUrls) ))

# gets all the asins that have been scraped
asin_found = list(file_handle.read_file(file_asinList,','))
print('asin_found total: ' + str(len(asin_found)))

"""# For search page scraping"""

searchedUrls=list(searchedUrls)
def getHeaders(info=None):

    if info != None:
        ref = info['ref']
        agentUse = info['agent']
    else:

        num = len(user_agents)
        posi = random.randint(1, num)
        age = list(user_agents)
        agentUse =  age[posi]

        da = random.randint(1,10)
        if (da % 2) == 0:
            lg = len(searchedUrls)
            rand = random.randint(0, int(lg))
            if lg > rand:
                ref = searchedUrls[rand]
            else:
                ref = searchedUrls[lg-1]
        else:
            ref = 'https://www.amazon.com'

    headers = {'dnt': '1','upgrade-insecure-requests': '1','user-agent': agentUse,'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9','sec-fetch-site': 'same-origin','sec-fetch-mode': 'navigate','sec-fetch-user': '?1','sec-fetch-dest': 'document','referer': ref,'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',}
    # print(headers)
    return [headers,agentUse]

def searchPg_info(url, agent, content):
    if content:
        searchData = []
        productsList = content.select('#search > div.s-desktop-width-max.s-opposite-dir > div > div.s-matching-dir.sg-col-16-of-20.sg-col.sg-col-8-of-12.sg-col-12-of-16 > div > span:nth-child(4) > div.s-main-slot.s-result-list.s-search-results.sg-row > div')
        for product in productsList:
            asin = product.get('data-asin')
            if asin != '':
                if asin not in asin_found:
                    asin_found.append(asin)
                    file_handle.write_in(file_asinList,asin,',')
                    searchData.append(str(product))

        if content.select('ul > li.a-last > a'):
            pagination = 'https://www.amazon.com' + str(content.select('ul > li.a-last > a')[0].get('href'))
        else :
            pagination = None

        data = {'url':url, 'user-agent':agent, 'search':list(searchData), 'next_page':pagination}
        file_handle.write_in(file_search_page, data, '\n')
        if pagination:
            headerUrl = {'ref':url, 'agent':agent}
            requester(pagination, 'storage_search', headerUrl)

def pagination(pg, url):
    pagi= pg.select('ul.a-pagination')
    if pagi != []:
        pagi= pg.select('ul.a-pagination')[0]
        if len( pagi.select('li.a-disabled') ) >= 3 and pagi.select('li.a-disabled')[1].text == '...':
          numOfPgs=pagi.select('li.a-disabled')[2].text
          links = pagi.findAll('a')
          for link in links:
              if 'Next' in link.text:
                  nextpg=link.get('href')
                  for i in range(2, int(numOfPgs)+1):
                      t=nextpg.replace('page=2',f'page={i}')
                      t=t.replace('sr_pg_1', f'sr_pg_{i-1}')

                      r=nextpg.replace('page=2',f'page={i-1}')
                      if i > 2:
                        r=r.replace('sr_pg_1', f'sr_pg_{i-2}')
                      else:
                        r=url
                      requester(f'https://www.amazon.com{t}','pagination',r)
        else:
          lst_pg = pagi.select('li.a-last a')[0].get('href')
          requester(f'https://www.amazon.com{lst_pg}','pagination',url)

def get_search(pg, ref, agent):
    title=''
    ratings=''
    link=''
    price=''
    stars=''
    image=''
    imgAlt=''

    searchlist=pg.findAll('div', {'data-asin': True, 'data-component-type':'s-search-result'})
    for product in searchlist:
        asin=product.get('data-asin')
        if asin not in asins_list:
            if product.select('h2 a span') != []:
                title=product.select('h2 a span')[0].text
                if title!='':
                    asins_list.append(asin)
                    file_handle.write_in(file_asinList, asin)
                    if product.select('span.a-offscreen') != [] :
                        price=product.select('span.a-offscreen')[0].text

                    if product.select('h2 a') != [] :
                        link='https://www.amazon.com'+str(product.select('h2 a')[0].get('href'))

                    if product.select('a span.a-size-base') != [] :
                        stars=product.select('a span.a-size-base')[0].text

                    if product.select('a span.a-size-base') != [] :
                        ratings=product.select('div.a-section.a-spacing-none.a-spacing-top-micro div.a-row.a-size-small span[aria-label]')[0].get('aria-label')

                    if product.select('img') != [] :
                        uml= product.select('img')[0]
                        image=uml.get('src')
                        imgAlt=uml.get('alt')
                        srcSets=uml.get('srcset').split(',')

                        Lo = []
                        for srcset in srcSets:
                            t = srcset.strip().split(" ")
                            da = {'imglink':t[0],'size':t[1]}
                            Lo.append(da)

                    data={'ref':ref,'user-agent':agent, 'asin':asin, 'title':title, 'price':price, 'link':link, 'people rated':stars, 'ratings':ratings, 'img':image, 'alt':imgAlt, 'srcsets':Lo}
                    file_handle.write_in(file_searched_products,data)

def product_details(pg, url):

    title=product.select('#producttitle')[0].text.replace("\n","")
    price=pg.find('span',{'class':'priceBlockStrikePriceString'}).text
    store=pg.find('a', {'id':'bylineInfo'})
    storelink='https://www.amazon.com'+store.get('href')
    storename=store.text.replace('Visit the', '')
    question=pg.find('a', {'class':'askATFLink'})
    noQuest=question.text.strip()
    questionlink='https://www.amazon.com'+question.get('href')

    data={'title':title,'price':price,'productby':{'store':storename,'link':storelink},'questions':{'no of questions':noQuest,'link':questionlink},}
    print(data)

def getProxy():
    ip = '.'.join(map(str,(random.randint(0,225) for _ in range(4))))+':'+str(random.randint(0,65))
    https = f'https://{ip}'
    http = f'http://{ip}'
    data = {'https': https,'http': http}
    return data

def requester(url, utype=None, headerUrl=None):
    if url not in searchedUrls:
        if headerUrl==None:
            header = getHeaders()
        else:
            header= getHeaders(headerUrl)

        url=url.replace(" ",'+')
        #proxies=getProxy()
        r = requests.get(url, headers=header[0])
        if r.status_code == 200:
            html =r.text
            file_handle.write_in(file_searchedList,url)
            searchedUrls.append(url)
            pg = soup(html, 'html.parser')
            if utype=='search':
                get_search(pg, url, header[1])
                pagination(pg,url)
            elif utype=='pagination':
                get_search(pg, url, header[1])
            elif utype=='product':
                product_details(pg, url)
            elif utype=='storage_search':
                searchPg_info(url, header[1], pg)
            else:
                return r.text
        else:
            print(r.status_code)

def search_data_scraper(url=None, Stype: str=None):
    if Stype !=None:
        sr=Stype
    else:
        sr='search'

    if url == None:
        threads = []
        for term in list(search_keys):
            url=str(f'https://www.amazon.com/s?k={term}')
            t = threading.Thread(target=requester, args=(str(url),str(sr)))
            t.daemon = True
            threads.append(t)

        for thread in threads:
            thread.start()

        for thread in threads:
            thread.join()
    else:
        if 'https://www.amazon.com/s?k=' in url:
            requester(url,sr)
        elif re.search("[0-9A-Za-z ]+", url):
            requester(f'https://www.amazon.com/s?k={url}',sr)
        else:
            print('Give a valid asin or a search url else leave it empty')

search_data_scraper(Stype='storage_search')